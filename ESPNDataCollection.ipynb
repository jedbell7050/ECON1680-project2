{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac0e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d80f2ef",
   "metadata": {},
   "source": [
    "***Warning this whole script takes 1 hour 45 minutes to run.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c183bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the URL format to get all the pages of the archive\n",
    "\n",
    "months = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', \n",
    "          'november', 'december']\n",
    "years = range(2016,2023,1)\n",
    "\n",
    "nba_urls = []\n",
    "for y in years:\n",
    "    for m in months:\n",
    "        if (m == 'may' and y == 2022):\n",
    "            break\n",
    "        nba_urls.append('http://www.espn.com/nba/news/archive/_/month/'+ m +'/year/'+str(y))\n",
    "        \n",
    "ncaab_urls = []\n",
    "for y in years:\n",
    "    for m in months:\n",
    "        if (m == 'may' and y == 2022):\n",
    "            break\n",
    "        ncaab_urls.append('http://www.espn.com/mens-college-basketball/news/archive?month='+m+'&year='+str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764e5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15-20 minutes\n",
    "nba_story_links = []\n",
    "stories_url_nba = []\n",
    "\n",
    "# Loop through every link on each archive page\n",
    "for url in nba_urls:\n",
    "    # Initialize a counter to count all the articles\n",
    "    counter = 0\n",
    "    r = requests.get(url)\n",
    "    page = BeautifulSoup(r.content, 'html.parser')\n",
    "    try:\n",
    "        # If the ul tag exists on a page, scrape all the urls in that list that contain the string 'story'\n",
    "        for item in page.find('ul').find_all('a'):\n",
    "            if 'story' in item.get('href'):\n",
    "                nba_story_links.append(item.get('href'))\n",
    "                counter += 1\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    stories_url_nba.append((url, counter))\n",
    "    \n",
    "# Repeat the same process for men's NCAA articles\n",
    "ncaab_story_links = []\n",
    "stories_url_ncaa = []\n",
    "for url in ncaab_urls:\n",
    "    counter = 0\n",
    "    r = requests.get(url)\n",
    "    page = BeautifulSoup(r.content, 'html.parser')\n",
    "    try:\n",
    "        for item in page.find('ul').find_all('a'):\n",
    "            if 'story' in item.get('href'):\n",
    "                ncaab_story_links.append(item.get('href'))\n",
    "                counter += 1\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    stories_url_ncaa.append((url, counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "356dae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the lists\n",
    "mbball_stories = nba_story_links\n",
    "mbball_stories.extend(ncaab_story_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ab7cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the women's basketball archive pages\n",
    "espnw_links = []\n",
    "for y in years: \n",
    "    for m in months:\n",
    "        if (m == 'may' and y == 2022):\n",
    "            break\n",
    "        espnw_links.append('http://www.espn.com/womens-college-basketball/news/archive?month='+m+'&year='+str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b27b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the same link getting process\n",
    "wnba_story_links = []\n",
    "story_url_wnba = []\n",
    "for url in espnw_links:\n",
    "    counter = 0\n",
    "    r = requests.get(url)\n",
    "    page = BeautifulSoup(r.content, 'html.parser')\n",
    "    try: \n",
    "        for item in page.find('ul').find_all('a'):\n",
    "            if 'story' in item.get('href'):\n",
    "                wnba_story_links.append(item.get('href'))\n",
    "                counter += 1\n",
    "\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    story_url_wnba.append((url, counter))\n",
    "        \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a790be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 45 Minutes\n",
    "\n",
    "# Initialize lists for article text, authors and dates\n",
    "w_articles = []\n",
    "w_authors = []\n",
    "w_date = []\n",
    "\n",
    "# Loop through each article link\n",
    "for url in wnba_story_links:\n",
    "    r = requests.get(url)\n",
    "    page = BeautifulSoup(r.content, 'html.parser')\n",
    "    body = page.find(class_='article-body')\n",
    "    \n",
    "    # Append the text to the articles list\n",
    "    w_articles.append(page.get_text())\n",
    "    try:\n",
    "        # If the author class exists, append it\n",
    "        w_authors.append(page.find(class_='author').get_text())\n",
    "    except AttributeError:\n",
    "        w_authors.append(np.nan)\n",
    "    \n",
    "    try:\n",
    "        # If the timestamp class exists, append it\n",
    "        date = body.find(class_='timestamp')\n",
    "        w_date.append(date.get_text())\n",
    "    except AttributeError:\n",
    "        w_date.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53974366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create women's dataframe\n",
    "dfw = pd.DataFrame({\"URL\":wnba_story_links, \"Author\":w_authors, \"Date\":w_date, \"Text\":w_articles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45661f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make F=1 (changed later when I realized not every article in the women's archive was necessarily about a woman)\n",
    "dfw['F'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18ff574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to save computation time\n",
    "dfw.to_csv('espn_wbball_stories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29008fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random subset of size = the number of women's articles (3577) for the men's articles\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1680)\n",
    "m_links = []\n",
    "for i in range(len(w_articles)):\n",
    "    # Generate random number to be the index\n",
    "    U = np.random.randint(0, len(mbball_stories))\n",
    "    m_links.append(nba_story_links[U])\n",
    "    # Remove the index so we don't get any duplicates\n",
    "    mbball_stories.pop(U)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c03ace0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 45 minutes\n",
    "\n",
    "# Repeat the article scraping process with the random sample of men's articles\n",
    "m_articles = []\n",
    "m_authors = []\n",
    "m_date = []\n",
    "for url in m_links:\n",
    "    r = requests.get(url)\n",
    "    page = BeautifulSoup(r.content, 'html.parser')\n",
    "    body = page.find(class_='article-body')\n",
    "    m_articles.append(page.get_text())\n",
    "    try:\n",
    "        m_authors.append(page.find(class_='author').get_text())\n",
    "    except AttributeError:\n",
    "        m_authors.append(np.nan)\n",
    "    \n",
    "    try:\n",
    "        date = body.find(class_='timestamp')\n",
    "        m_date.append(date.get_text())\n",
    "    except AttributeError:\n",
    "        m_date.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83725d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of men's articles\n",
    "dfm = pd.DataFrame({\"URL\":m_links, \"Author\":m_authors,\"Date\":m_date, \"Text\":m_articles})\n",
    "dfm['F'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a6aaae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "dfm.to_csv('espn_mbball_stories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1265a0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Women's Articles: 3576\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Women's Articles:\", len(set(wnba_story_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce253bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Men's Articles: 19809\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Men's Articles:\", len(set(mbball_stories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81fd293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
